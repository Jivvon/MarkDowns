# Distance Metric Types

> 유클리드 정리
>
> 맨하탄 정리
>
> 민코프스키 거리
>
> 놈

## Pairwise : (1 to 1)

- **피타고라스 정리**
- **Euclidean Distance (Euclidean Geometry) : 유클리드 공간에서의 직선 거리.**
  - n Dimension : $$d(p,q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_n-q_n)^2}$$
- **Manhattan Distance (Taxicab Geometry)**
  - $$d_1(p,q) = ||p-q||_1 (norm) = \Sigma_{i=1}^{n}{|p_i-q_i|}$$
- **Minkowski distance** : metric in a **normed vector** space which can be considered as a **generalization** of both the <u>Euclidean distance</u> and the <u>Manhattan distance</u> 즉, 택시캡 distance와 유클리드 distance를 공식 하나로 합쳤다.

### Norm

아래의 세 가지 속성을 동시에 만족한다면 Norm( $\|v\|$ )이라 한다. 

vector값을 scalar값으로 바꿀 수 있는 function의 일종.

1. Zero Vector : $v = 0$ 
2. Scalar Factor : $f(cv) = |c|f(v)$
3. Triangle Inequality : $f(u + v) \le f(u) + f(v)$

##### Taxicab norm

##### Euclidean Space [Assignment]

##### P-norm

let $p \ge 1$ be a real number.

The $p-norm$(also called $l_p-norm$) of vector $X = (x_1,...,x_n)$

> $$||X||_p = (\Sigma_{i=1}^n{|x_i|^p})^{1/p}$$
>
> $p=1$ : Taxicab (Manhattan) norm ($l_1-norm$)
>
> $p=2$ :Euclidean norm ($l_2-norm$)
>
> $p=\infty$ : Maximum norm

##### Minkowski Distance

##### $X = (x_1,x_2,...,x_n)\text{and}Y = (y_1,y_2,...,y_n) \in \R^n$

> $$D(X,Y) = (\Sigma_{i=1}^n{|x_i-y_i|^p})^{1/p}$$
>
> $p=1$ : Taxicab (Manhattan) Distance ($l_1-distance$)
>
> $p=2$ :Euclidean Distance ($l_2-distance$)
>
> $p=\infty$ : Maximum norm

##### Cosine Distance

Dot product definition : $a\cdot b = \|a\|\|b\|cos\theta$

Cosine Distance : $1-cos\theta$

> **Cosin Similarity**
>
> $cos\theta = \|a\|\|b\| \over a•b$

## Distribution-wise : 1 (or Many) to Many 

> 마할라노비스 거리

### Mahalanobis Distance

##### Correlation (상호 관계) : x와 y가 연관되어 있다.

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-04-03 오후 7.04.45.png" alt="스크린샷 2020-04-03 오후 7.04.45" style="zoom:33%;" />

유클리디안의 단점을 보완하기 위해 scaling을 한다. 즉, unit variance를 가진다. (범위를 정규화하는 것)

> **problem of Euclidean Distance 유클리디안 거리의 단점.**
>
> 의미적으로 같은 자료라도 scale이 다르면 distance가 다르게 나옴.

##### 마할라노비스 Distance의 Big picture

> 1. 각 Component들 사이의 상관관계를 없앤다.
> 2. Scale에 민감하지 않게 정규화해준다.
> 3. Euclidean처럼 거리를 계산한다.

### 공식 : $D_M(x,y) = \sqrt{(x-y)^TS^{-1}(x-y)}$

$x$ : **vector** of the data point (dimension에 따라 x와 y 벡터의 col 개수가 달라진다)

$y$ : **vector** of the data point

$S$ : covariance matrix (공분산 행렬)

$S^{-1}$ : inverse matrix of covariance matrix

> 아래 두 식은 같은 식이다. (Euclidean Distance)
>
> $D_{L2}(x,y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$
>
> $D_{L2}(x,y) = \sqrt{(x-y)^T(x-y)}$

### Euclidean vs Mahalanobis

**Euclidean** : $D_{L2}(x,y) = \sqrt{(x-y)^T(x-y)}$

**Mahalanobis Distance to Center** : $D_M(x,\mu) = \sqrt{(x-\mu)^TC^{-1}(x-\mu)}$

**Mahalanobis Distance Generalized form** : $D_M(x,y) = \sqrt{(x-y)^TS^{-1} (x-y)}$

$x-\mu$ in M to Center 은 **각 component의 중심까지의 거리**

$C^{-1}$ in M to Center 은 **상관관계를 없애고 Scale factor를 정규화한다.**

> **Covariance Matrix** ($S^{-1}$)
>
> $x = (x_1, x_2, x_3)$
>
> $x_1$이 변할 때 $x_2,x_3$가 변하는 정도를 확인. : n x n의 모든 선형 관계를 측정한다. Covariance로.
>
> $Covariance(x_1,x_2) = cov(x_1,x_2)$
>
> <img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-04-03 오후 7.43.10.png" alt="스크린샷 2020-04-03 오후 7.43.10" style="zoom:33%;" />

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-04-03 오후 7.51.55.png" alt="스크린샷 2020-04-03 오후 7.51.55" style="zoom:30%;" />   

$\sigma$는 **폭**이라 생각하자. (feat. 직관)  

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-04-03 오후 7.54.02.png" alt="스크린샷 2020-04-03 오후 7.54.02" style="zoom:30%;" />

공식을 보면 **공분산 행렬의 역행렬**을 중간에 곱해준다. 

​	-> 이거 결국 **단위 행렬의 각 행에 분산을 나누는 것**과 일치함.

아래는 **no correlation**일 때의 마할라노비스 공식.

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-04-03 오후 8.05.49.png" alt="스크린샷 2020-04-03 오후 8.05.49" style="zoom:34%;" />




