# Principal Component Analysis

> Dimension Reduction
>

## Dimension Reduction

높은 차원의 데이터 중에서 몇 가지만 골라서 새로운 (더 작은) 차원의 데이터를 만드는 것.

- Feature Selection
  
  - 가공하지 않고 그대로 selection해서 사용한다.
- **Feature Projection** (Feature extraction) (generally)

  - selection 후 가공하여 사용한다.

  - > **Principal component analysis (PCA)**
    >
    > Non-negative matrix factorization (NMF)
    >
    > Linear discriminant analysis (LDA)
    >
    > Autoencoder
    >
    > t-SNE
    >
    > UMAP

## Principal component analysis (PCA)

##### 선형변환을 통해 현상을 더 잘 설명할 수 있는 데이터 축을 찾아내 보자

1. ${X}\rightarrow{Y}$ : Linear Data Projection
2. Basis Change : Linear Projection할 때 데이터가 잘 설명될 수 있는 축으로 basis를 바꿀 것이다.
3. Minimize Redundancy : Data를 사용하는 component들 끼리의 정보 중복성(redundancy)를 줄인다.

#### Linear Transformation 선형 변환

##### Simply rotation + stretch

어떠한 matrix에 의해 방향이 바뀌지 않는 vector들이 있다. (크기만 바뀐다) = 아이젠벡터

> If Every Vector is Eigenvector, then Matrix is a Multiple of Identity Matrix

##### Intuitive Understanding 직관적 이해

correlated : A가 움직일 때, D도 따라서 움직인다면 **A와 D 중 하나의 정보만 필요하다**

not-correlated : A가 움직일 때, D의 움직임이 별 관계가 없다면 **A와 D 둘 모두의 정보가 필요하다**

### PX = Y

${X}\rightarrow^P{Y}$ 에서 X는 Redundant Components, Y는 Not-redundant Components.

즉, Y의 모든 컴포넌트는 decorrelated하다. **Covariance of Y ($C_Y$=0)**

All off-diagonal terms in $C_Y$ should be **zero**. Thus, *$C_Y$ must be a diagonal matrix.*



###### P를 구해 보자

> original data matrix X의 covariance matrix $C_X$를 구한다.
>
> $C_X$의 $E·v (Eigenvector)$를 구한다. (여러개)
>
> 각각의 Eigenvector에서 $e·v(Eigen value)$를 구하여 순서대로 marix를 만들면 P가 된다.

<img src="/Users/jiwon/Library/Application Support/typora-user-images/image-20200421094443857.png" alt="image-20200421094443857" style="zoom:50%;" />

<img src="/Users/jiwon/Library/Application Support/typora-user-images/image-20200421094624684.png" alt="image-20200421094624684" style="zoom:50%;" />

아이젠 벡터를 큰 순서대로 나열한 후, 영향력이 작은 하위 n(여기선 -2)를 날린다.

<img src="/Users/jiwon/Library/Application Support/typora-user-images/image-20200421094840668.png" alt="image-20200421094840668" style="zoom:50%;" />

X를 가장 잘 설명할 수 있는 Y를 구할 수 있다. **(X를 가장 잘 설명할 수 있는 basis로 바뀌어 Y가 되었다.)**








