# 201502119 정지원 lab06 보고서

### Goal1 : 데이터 전처리

```python
def get_data(path):
    data = pd.read_csv(path)
    data = data.drop(['Name', 'Cabin', 'Ticket'], axis=1)
    data['Sex'] = pd.get_dummies(data['Sex']) # Sex는 0과 1로 바꾼다.
    # Age가 nan인 경우 평균 나이값으로 대체한다.
    data['Age'] = data['Age'].fillna(np.round(np.nanmean(data['Age'].values)))
    # Fare가 nan인 경우 Fare의 최빈값으로 대체한다.
    data['Fare'] = data['Fare'].fillna(data['Fare'].value_counts().keys()[0]) 
    # Embarked가 nan인 경우 Fare의 최빈값으로 대체한다.
    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].value_counts().keys()[0])
    # Embarked는 C,Q,S를 각각 0,1,2에 매핑한다.
    data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})
```

##### 실행결과

<div style="display:flex;flex-flow:row nowrap;">
	<div>
		<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-03 오전 12.48.32.png" alt="스크린샷 2020-05-03 오전 12.48.32" style="zoom:50%;" />
	</div>  
	<div>
		<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-03 오전 12.50.00.png" alt="스크린샷 2020-05-03 오전 12.50.00" style="zoom:50%;" />
	</div>
</div>
좌측이 원본 데이터, 우측이 전처리 후의 데이터이다.

1. 전처리 과정에서 'Name', 'Cabin', 'Ticket' columns를 제거하였다.
2. 'Sex', 'Embarked'의 type을 숫자로 변형하였다.
3. Age가 nan인 경우 평균 나이값으로 대체하였다.
4. Fare가 nan인 경우 최빈값으로 대체하였다.
5. Embarked가 nan인 경우 최빈값으로 대체하였다.




### Goal2 : Decision Tree 활용 및 시각화

train_test_split()을 사용해서 overfitting을 방지한다. 이 때 train_test_split으로 트레이닝 데이터를 적당한 크기로 나누는 것이고 여긴 생존정보가 들어가야 한다. 리턴값을 train, test로 받으면 생존정보를 포함한 데이터가 train과 test에 저장되고, 이를 이용해서 의사결정트리를 만든다. 이후 이 의사결정트리를 이용해서 문제에서 제공한 (생존정보가 없는)test.csv를 예측한다.

```python
if __name__ == "__main__":
    train, test = train_test_split(train_data)
    x_train = train.drop('Survived', axis=1) # 생존 정보 제외
    y_train = train['Survived'] # 생존 정보
    features = x_train.keys()
    x_test = test[features] # 생존 정보 제외된 test.csv 데이터

    dt = run_dt(x_train, y_train, x_test, features)
```
```python
def run_dt(x_train, y_train, x_test, features):
    dt = DecisionTreeClassifier(max_depth=3)
    dt.fit(x_train, y_train)
    predict_result = dt.predict(x_test)

    # 시각화 코드
    export_graphviz(dt, out_file="dt.dot", class_names=['No', 'Yes'],
                    feature_names=features, impurity=False, filled=True)
    (graph, ) = pydot.graph_from_dot_file('dt.dot', encoding='utf-8')
    graph.write_png('dt.png')

    return dt
```



<img src="/Users/jiwon/GoogleDrive/201502119/Lab06_201502119/dt_nonLimitDepth.png" alt="dt_nonLimitDepth" style="zoom:50%;" />

의사결정트리의 깊이 제한 없이 만들었을 때의 트리이다. 트리의 깊이가 불필요하게 굉장히 깊은 것을 알 수 있고, 자세히 보면 너무 자세히 분류되어 있다.

<img src="/Users/jiwon/GoogleDrive/201502119/Lab06_201502119/dt.png" alt="dt" style="zoom:40%;" />

그래서 maxdepth=3으로 최종적으로 제한을 걸어두고 의사결정트리를 만들었다.

> 나이를 포함한 다른 값들도 **숫자를 범위로 지정하여 재분류하면** 더 좋은 결과를 얻을 수 있을 것 같다.



### Goal3 : Kaggle에 제출하여 점수 확인하기

<img src="/Users/jiwon/Library/Application Support/typora-user-images/image-20200503004619583.png" alt="image-20200503004619583" style="zoom:50%;" />

의사결정트리의 최대깊이를 다르게하여 제출해보았다.

Goal2에서 본 것처럼 깊이가 깊다고 좋은 결과가 나오는 것이 아님을 알 수 있다.

> 나이를 포함한 다른 값들도 **숫자를 범위로 지정하여 재분류하면** 더 좋은 결과를 얻을 수 있을 것 같다.