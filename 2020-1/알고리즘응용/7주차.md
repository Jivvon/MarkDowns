# DNN

> Representation Learning
>
> Deep Learning - Representation Learning View
>
> Deep Learning - Data Transformation View

## Representation Learning

#### Distributed Representation

DNN가 기존 AI 방법론들에 비해 큰 의미가 있는 점은 실세계에 있는 실제 객체를 표현할 때 Symbol에 의존하지 않는다는 것이다.

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-01 오후 5.19.36.png" alt="스크린샷 2020-05-01 오후 5.19.36" style="zoom:50%;" />

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-01 오후 5.21.42.png" alt="스크린샷 2020-05-01 오후 5.21.42" style="zoom:80%;" />



# 어떻게 DNN은 사물의 특징을 파악할 수 있을까 ?

### Latent Variable (Hidden Variable)

- Deep Neural Network의 핵심
- 숫자이지만, 현실세계에서 counting할 수 없는 숫자이다. ($h$ : hidden variable)

##### $x$ : 실세계에 존재하는 관측 가능한 것

##### $h$ : 현실세계에 존재하지 않는 가상의 값이며 간적접인 추측만 가능. (즉, 무엇이든 될 수 있는 값)

###### $h$를 원하는 방식으로 찾아보자.

1. 많은 수의 데이터
2. 구조적 연관성
   1. 관측 가능한 값 x와 묶어서 비슷한 성질을 갖도록 하자
   2. x와 y가 나타날 때마다 비슷한 성질을 갖는 h를 갖도록 하자
   3. x, y, z, z1, z2... 수많은 변수를 가지고 h를 유의미하게 할 수 있다.

---



Batch Gradient Descent : use all *m* examples in each iteration

Stochastic Gradient Descent : use *1* examples in each iteration

Mini-batch Gradient Descent : use *b* examples in each iteration

​	Batch와 Stochastic의 중간. 즉, Batch size 만큼만 update에 활용한다.



에폭 : 데이터를 몇 번 봤냐



Loss : 정답과 예측이 얼마나 다른가 (Loss = **Objective Function** = Error = Cost Function)

- 큰 문제가 아니라면 0에 수렴해야 한다.

Learning Rate : $\alpha * n + \theta = \theta^\prime$ 에서의 $\alpha$ 값이다.



##### Overfiting



---

## Statistical Machine Learning

- **Data Preparation** : 가지고 있는 데이터를 이용하여
  - file $\rightarrow$ Tensor
- Model Implemetation : 풀고자 하는 문제의 통계적 모델링을 통해
- Loss Implementation : 실제 정답과의 오차를
  - 정답과 예측값의 오차율
- Updater Implementation : 파라미터 학습을 통해
  - 오차를 줄이는 방향으로 (최적화)
- **Iterative Learning** : 줄이는 과정을 반복



### Data Preparation

딥러닝은 tensor를 가지고 노는 것이라고 할 수도 있다.

- **Tensor**
- **Epoch**
- **Batch**



### Training

- 방식
  - Batch
  - Stochastic Gradient Descent
  - **Mini-batch Gradient Descent** : (generally) Batch size 만큼만 update에 활용한다. 한번의 iteration이 끝나면 다음 예제로 넘어간다.
- **Epoch** : 모든 데이터를 살펴보는 횟수. (한번 살피면 1epoch)
- **Step** : 파라미터를 update하는 횟수.

> 10개의 데이터에서 batch size가 3이라면 4 step이 되어야 1 epoch이 된다.





### Tuning Learning Rate

**Learning Rate** : $\alpha * n + \theta = \theta^\prime$ 에서의 $\alpha$ 값이다.

- loss값을 관찰하면서 그 때에 맞는 learning rate를 적절하게 적용해야 한다.
- overfitting일 경우 수정해야 한다



### Train / Validation Accuracy

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-05 오후 5.09.20.png" alt="스크린샷 2020-05-05 오후 5.09.20" style="zoom:50%;" />



> ###### Training Data : Validation Data : Test Data 가 보통 6:2:2 로 된다.
>
> > Training Data로 1-epoch정도 돌려보고 Validation Data로 테스트해보며 대략적인 성능을 모니터링한다. 이를 통해 나온 모델을 Test Data에 돌려 최종적인 결과물을 산출한다.
>
> accuracy는 Training Data가 가장 좋을 것. (훈련용 데이터이기 때문.)
>
> Training Accuracy가 증가하는 만큼 Validation Accuracy가 증가하지 않으면 모델이 overfitting된 것이므로 learning rate를 적절하게 변경한다.









