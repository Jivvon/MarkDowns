# DNN

> Representation Learning
>
> Deep Learning - Representation Learning View
>
> Deep Learning - Data Transformation View

## Representation Learning

#### Distributed Representation

DNN가 기존 AI 방법론들에 비해 큰 의미가 있는 점은 실세계에 있는 실제 객체를 표현할 때 Symbol에 의존하지 않는다는 것이다.

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-01 오후 5.19.36.png" alt="스크린샷 2020-05-01 오후 5.19.36" style="zoom:50%;" />

<img src="/Users/jiwon/Library/Application Support/typora-user-images/스크린샷 2020-05-01 오후 5.21.42.png" alt="스크린샷 2020-05-01 오후 5.21.42" style="zoom:80%;" />



# 어떻게 DNN은 사물의 특징을 파악할 수 있을까 ?

### Latent Variable (Hidden Variable)

- Deep Neural Network의 핵심
- 숫자이지만, 현실세계에서 counting할 수 없는 숫자이다. ($h$ : hidden variable)

##### $x$ : 실세계에 존재하는 관측 가능한 것

##### $h$ : 현실세계에 존재하지 않는 가상의 값이며 간적접인 추측만 가능. (즉, 무엇이든 될 수 있는 값)

###### $h$를 원하는 방식으로 찾아보자.

1. 많은 수의 데이터
2. 구조적 연관성
   1. 관측 가능한 값 x와 묶어서 비슷한 성질을 갖도록 하자
   2. x와 y가 나타날 때마다 비슷한 성질을 갖는 h를 갖도록 하자
   3. x, y, z, z1, z2... 수많은 변수를 가지고 h를 유의미하게 할 수 있다.

---



Batch Gradient Descent : use all *m* examples in each iteration

Stochastic Gradient Descent : use *1* examples in each iteration

Mini-batch Gradient Descent : use *b* examples in each iteration

​	Batch와 Stochastic의 중간. 즉, Batch size 만큼만 update에 활용한다.



에폭 : 데이터를 몇 번 봤냐



Loss : 정답과 예측이 얼마나 다른가 (Loss = Objective Function = Error = Cost Function)

- 큰 문제가 아니라면 0에 수렴해야 한다.

Learning Rate : $\alpha * n + \theta = \theta^\prime$ 에서의 $\alpha$ 값이다.



##### Overfiting

























